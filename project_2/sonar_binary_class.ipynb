{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1664504176619",
   "display_name": "Python 3.7.12 64-bit ('deeplearning': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# The Dataset\n",
    "The dataset you will use in this chapter is the Sonar dataset. This is a dataset that describes\n",
    "sonar chirp returns bouncing off different surfaces. The 60 input variables are the strength of\n",
    "the returns at different angles. It is a binary classification problem that requires a model to\n",
    "differentiate rocks from metal cylinders.\n",
    "It is a well-understood dataset. All the variables are continuous and generally in the\n",
    "range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need\n",
    "to be converted to integers 1 and 0 as neural networks can only output numbers.\n",
    "\n",
    "A benefit of using this dataset is that it is a standard benchmark problem. This means\n",
    "that we have some idea of the expected skill of a good model. Using cross-validation, a neural\n",
    "network should be able to achieve a performance of around 84% with an upper bound on\n",
    "accuracy for custom models at around 88%. You can learn more about this dataset on the\n",
    "UCI Machine Learning repository.\n",
    "\n",
    "# Baseline Neural Network Model performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv(\"sonar.all-data\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "source": [
    "You are now ready to create your neural network model using Keras. You will use scikit-learn\n",
    "to evaluate the model using stratified k-fold cross-validation. This is a resampling technique\n",
    "that will provide an estimate of the performance of the model. It does this by splitting the\n",
    "data into k parts and training the model on all parts except one, which is held out as a test set\n",
    "to evaluate the performance of the model. This process is repeated k times, and the average\n",
    "score across all constructed models is used as a robust estimate of performance. It is stratified,\n",
    "meaning that it will look at the output values and attempt to balance the number of instances\n",
    "that belong to each class in the k splits of the data.\n",
    "\n",
    "To use Keras models with scikit-learn, you must use the `KerasClassifier wrapper` from\n",
    "the SciKeras module. This class takes a function that creates and returns our neural network\n",
    "model. It also takes arguments that it will pass along to the call to `fit()`, such as the number\n",
    "of epochs and the batch size.\n",
    "\n",
    "Let’s start by defining the function that creates your baseline model. Your model will\n",
    "have a single, fully connected hidden layer with the same number of neurons as input variables.\n",
    "This is a good default starting point when creating neural networks.\n",
    "\n",
    "The weights are initialized using a small Gaussian random number. The Rectifier\n",
    "activation function is used. The output layer contains a single neuron in order to make\n",
    "predictions. It uses the sigmoid activation function in order to produce a probability output\n",
    "in the range of 0 to 1 that can easily and automatically be converted to crisp class values.\n",
    "\n",
    "Finally, you will use the logarithmic loss function `(binary_crossentropy)` during training,\n",
    "the preferred loss function for binary classification problems. The model also uses the efficient\n",
    "Adam optimization algorithm for gradient descent, and accuracy metrics will be collected\n",
    "when the model is trained."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "source": [
    "Now, it is time to evaluate this model using stratified cross-validation in the scikit-learn\n",
    "framework. Pass the number of training epochs to the `KerasClassifier`, again using reasonable\n",
    "default values. Verbose output is also turned off, given that the model will be created ten\n",
    "times for the 10-fold cross-validation being performed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2022-09-29 22:58:22.053030: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-09-29 22:58:22.053104: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n2022-09-29 22:58:22.053138: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (theia-20220907-182918): /proc/driver/nvidia/version does not exist\n2022-09-29 22:58:22.053557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nBaseline: 82.24% (6.34%)\n"
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "source": [
    "# Improve the Baseline Model with Data Preparation\n",
    "It is a good practice to prepare your data before modeling. Neural network models are especially\n",
    "suitable for having consistent input values, both in scale and distribution. Standardization is\n",
    "an effective data preparation scheme for tabular data when building neural network models.\n",
    "This is where the data is rescaled such that the mean value for each attribute is 0, and\n",
    "the standard deviation is 1. This preserves Gaussian and Gaussian-like distributions while\n",
    "normalizing the central tendencies for each attribute. You can use scikit-learn to perform the\n",
    "standardization of your sonar dataset using the `StandardScaler` class.\n",
    "\n",
    "Rather than performing the standardization on the entire dataset, it is good practice to\n",
    "train the standardization procedure on the training data within the pass of a cross-validation\n",
    "run, and use the trained standardization instance to prepare the unseen test fold. This makes\n",
    "standardization a step in model preparation in the cross-validation process. It prevents the\n",
    "algorithm from having knowledge of unseen data during evaluation, knowledge that might be\n",
    "passed from the data preparation scheme like a crisper distribution.\n",
    "\n",
    "You can achieve this in scikit-learn using a `Pipeline` class. The pipeline is a wrapper that\n",
    "executes one or more models within a pass of the cross-validation procedure. Here, you can\n",
    "define a pipeline with the `StandardScaler` followed by your neural network model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2022-09-30 02:23:36.510443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-09-30 02:23:36.510578: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n2022-09-30 02:23:36.510629: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (theia-20220907-182918): /proc/driver/nvidia/version does not exist\n2022-09-30 02:23:36.511620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nStandardized: 87.55% (6.76%)\n"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv(\"sonar.all-data\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', \n",
    "    KerasClassifier(model=create_baseline, \n",
    "        epochs=100, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "source": [
    "# Tuning Layers and Number of Neurons in the Model\n",
    "\n",
    "There are many things to tune on a neural network, such as weight initialization, activation\n",
    "functions, optimization procedure, and so on. One aspect that may have an outsized effect is\n",
    "the structure of the network itself, called the network topology. In this section, you will look at two experiments on the structure of the network: making it smaller and making it larger. These are good experiments to perform when tuning a neural network on your problem.\n",
    "\n",
    "## Evaluate a Smaller Network\n",
    "Note that there is likely a lot of redundancy in the input variables for this problem. The data\n",
    "describes the same signal from different angles. Perhaps some of those angles are more relevant\n",
    "than others. So you can force a type of feature extraction by the network by restricting the\n",
    "representational space in the first hidden layer.\n",
    "\n",
    "In this experiment, you will take your baseline model with 60 neurons in the hidden layer\n",
    "and reduce it by half to 30. This will pressure the network during training to pick out the\n",
    "most important structure in the input data to model.\n",
    "\n",
    "You will also standardize the data as in the previous experiment with data preparation\n",
    "and try to take advantage of the slight lift in performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Smaller: 83.19% (7.13%)\n"
    }
   ],
   "source": [
    "# smaller model\n",
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_smaller,\n",
    "epochs=100, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "source": [
    "## Evaluate a Larger Network\n",
    "A neural network topology with more layers offers more opportunities for the network to\n",
    "extract key features and recombine them in useful nonlinear ways. You can easily evaluate\n",
    "whether adding more layers to the network improves the performance by making another\n",
    "small tweak to the function used to create our model. Here, you add one new layer (one line)\n",
    "to the network that introduces another hidden layer with 30 neurons after the first hidden\n",
    "layer.\n",
    "\n",
    "The idea here is that the network is given the opportunity to model all input variables before\n",
    "being bottlenecked and forced to halve the representational capacity, much like you did in the\n",
    "experiment above with the smaller network. Instead of squeezing the representation of the\n",
    "inputs themselves, you have an additional hidden layer to aid in the process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Larger: 86.10% (7.59%)\n"
    }
   ],
   "source": [
    "# larger model\n",
    "def create_larger():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}