{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1664489003077",
   "display_name": "Python 3.7.12 64-bit ('deeplearning': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Prep\n",
    "Use the standard machine learning problem called the iris flowers\n",
    "dataset. This dataset is well studied and makes a good problem for practicing on neural\n",
    "networks because all four input variables are numeric and have the same scale in centimeters.\n",
    "Each instance describes the properties of an observed flower’s measurements, and the output\n",
    "variable is a specific iris species. The attributes for this dataset can be summarized as follows:\n",
    "\n",
    "1. Sepal length in centimeters\n",
    "2. Sepal width in centimeters\n",
    "3. Petal length in centimeters\n",
    "4. Petal width in centimeters\n",
    "5. Class (the flower species)\n",
    "\n",
    "This is a multiclass classification problem, meaning that there are more than two classes\n",
    "to be predicted. In fact, there are three flower species. This is an important problem for\n",
    "practicing with neural networks because the three class values require specialized handling.\n",
    "Below is a sample of the first five of the 150 instances:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n5.4,3.9,1.7,0.4,Iris-setosa\n4.6,3.4,1.4,0.3,Iris-setosa\n5.0,3.4,1.5,0.2,Iris-setosa\n4.4,2.9,1.4,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n"
    }
   ],
   "source": [
    "!head iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv(\"iris.data\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]"
   ]
  },
  {
   "source": [
    "The output variable contains three different string values. When modeling multiclass\n",
    "classification problems using neural networks, it is good practice to reshape the output attribute\n",
    "from a vector of class labels to a matrix of a Boolean for each class and whether a given instance\n",
    "is in that class. This is called one-hot encoding or creating dummy variables from a categorical\n",
    "variable. For example, in this problem, the three class values are `Iris-setosa`, `Iris-versicolor`,\n",
    "and `Iris-virginica`.\n",
    "\n",
    "You can first encode the strings consistently to integers using the scikit-learn class LabelEncoder.\n",
    "Then convert the vector of integers to a one-hot encoding using the Keras function\n",
    "to_categorical()."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one-hot encoded)\n",
    "dummy_y = to_categorical(encoded_Y)"
   ]
  },
  {
   "source": [
    "# Define the Neural Network Model\n",
    "The Keras library provides wrapper classes to allow you to use neural network models developed\n",
    "with Keras in scikit-learn. There is a `KerasClassifier` class in SciKeras that can be used as\n",
    "an estimator in scikit-learn, the base type of model in the library. The `KerasClassifier` takes\n",
    "the name of a function as an argument. This function must return the constructed neural\n",
    "network model, ready for training.\n",
    "Below is a function that will create a baseline neural network for the iris classification\n",
    "problem. It creates a simple, fully connected network with one hidden layer that contains\n",
    "eight neurons. The hidden layer uses a rectifier activation function which is a good practice.\n",
    "Because you used a one-hot encoding for your iris dataset, the output layer must create three\n",
    "output values, one for each class. The output value with the largest value will be taken as the\n",
    "class predicted by the model.\n",
    "\n",
    "Note that a `“softmax”` activation function was used in the output layer. This ensures\n",
    "the output values are in the range of 0 and 1 and may be used as predicted probabilities.\n",
    "Finally, the network uses the efficient `Adam gradient descent optimization algorithm` with a\n",
    "logarithmic loss function, which is called `“categorical_crossentropy”` in Keras.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_shape=(4,), activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  },
  {
   "source": [
    "You can now create your `KerasClassifier` for use in scikit-learn. You can also pass arguments\n",
    "in the construction of the `KerasClassifier` class that will be passed on to the `fit()` function\n",
    "internally used to train the neural network. Here, you pass the number of epochs as 200 and\n",
    "batch size as 5 to use when training the model. Debugging is also turned off when training\n",
    "by setting `verbose` to 0."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(model=baseline_model, epochs=200, batch_size=5, verbose=0)"
   ]
  },
  {
   "source": [
    "# Evaluate the Model with k-Fold Cross-Validation\n",
    "You can now evaluate the neural network model on our training data. The scikit-learn library\n",
    "has excellent capability to evaluate models using a suite of techniques. The gold standard\n",
    "for evaluating machine learning models is k-fold cross-validation. First, define the model\n",
    "evaluation procedure. Here, you set the number of folds to 10 (an excellent default) and\n",
    "shuffle the data before partitioning it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "source": [
    "Now, you can evaluate your model (`estimator`) on your dataset (X and dummy_y) using a 10-fold\n",
    "cross-validation procedure (`KFold`). Evaluating the model only takes approximately 10 seconds\n",
    "and returns an object that describes the evaluation of the ten constructed models for each of\n",
    "the splits of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2022-09-29 22:23:21.509832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-09-29 22:23:21.509897: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n2022-09-29 22:23:21.509951: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (theia-20220907-182918): /proc/driver/nvidia/version does not exist\n2022-09-29 22:23:21.510547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe019d61b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe01a63cc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nBaseline: 98.00% (3.06%)\n"
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}