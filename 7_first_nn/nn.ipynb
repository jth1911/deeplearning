{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1664426519606",
   "display_name": "Python 3.7.12 64-bit ('deeplearning': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Summary of steps:\n",
    "- Load Data\n",
    "- Define Keras Model\n",
    "- Compile Keras Model\n",
    "- Fit Keras Model\n",
    "- Evaluate Keras Model\n",
    "- Tie It All Together\n",
    "- Make Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n"
   ]
  },
  {
   "source": [
    "In this Keras tutorial, you will use the Pima Indians onset of diabetes dataset. This is a standard machine learning dataset from the UCI Machine Learning repository. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n",
    "\n",
    "It is a binary classification problem (onset of diabetes as 1 or not as 0). All of the input variables that describe each patient are numerical. This makes it easy to use directly with neural networks that expect numerical input and output values and is an ideal choice for our\n",
    "first neural network in Keras.\n",
    "\n",
    "The dataset is available from the bundle of sample code provided with this book. You can also download it here:\n",
    "- Dataset CSV File (https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
    "- Dataset Details (https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names)\n",
    "\n",
    "Download the dataset and place it in your local working directory, the same location as your. Python file. Save it with the filename pima-indians-diabetes.csv.\n",
    "\n",
    "```\n",
    "wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.data.csv', delimiter=',')\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:, 0:8]\n",
    "y = dataset[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define Keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "Training occurs over epochs, and each epoch is split into batches.\n",
    "- Epoch: One pass through all of the rows in the training dataset\n",
    "- Batch: One or more samples considered by the model within an epoch before weights\n",
    "are updated\n",
    "\n",
    "One epoch comprises of one or more batches, based on the chosen batch size, and the model is fit for many epochs. The training process will run for a fixed number of epochs (iterations) through the entire dataset that you must specify using the epochs argument. You must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size, and set using the batch_size argument. This problem will run for a small number of epochs (150) and use a relatively small batch size of 10. These configurations can be chosen experimentally by trial and error. You want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/150\n77/77 [==============================] - 1s 1ms/step - loss: 19.7082 - accuracy: 0.6419\nEpoch 2/150\n77/77 [==============================] - 0s 2ms/step - loss: 2.2713 - accuracy: 0.5508\nEpoch 3/150\n77/77 [==============================] - 0s 2ms/step - loss: 1.3593 - accuracy: 0.5599\nEpoch 4/150\n77/77 [==============================] - 0s 2ms/step - loss: 1.0924 - accuracy: 0.5703\nEpoch 5/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.9358 - accuracy: 0.5625\nEpoch 6/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.8735 - accuracy: 0.5807\nEpoch 7/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.8430 - accuracy: 0.5755\nEpoch 8/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.7953 - accuracy: 0.6328\nEpoch 9/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.7728 - accuracy: 0.6549\nEpoch 10/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.7433 - accuracy: 0.6667\nEpoch 11/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.7348 - accuracy: 0.6654\nEpoch 12/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.7032 - accuracy: 0.6732\nEpoch 13/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.7006 - accuracy: 0.6784\nEpoch 14/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.6771\nEpoch 15/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.6784\nEpoch 16/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6682 - accuracy: 0.6862\nEpoch 17/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.6836\nEpoch 18/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.6836\nEpoch 19/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6914\nEpoch 20/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.6836\nEpoch 21/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6914\nEpoch 22/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.6339 - accuracy: 0.6875\nEpoch 23/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6329 - accuracy: 0.6758\nEpoch 24/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.6940\nEpoch 25/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.6862\nEpoch 26/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.6115 - accuracy: 0.6966\nEpoch 27/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6091 - accuracy: 0.6927\nEpoch 28/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6065 - accuracy: 0.6914\nEpoch 29/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5971 - accuracy: 0.6875\nEpoch 30/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.6073 - accuracy: 0.6745\nEpoch 31/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5977 - accuracy: 0.6901\nEpoch 32/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.6875\nEpoch 33/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.7044\nEpoch 34/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.6901\nEpoch 35/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.6940\nEpoch 36/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5885 - accuracy: 0.6927\nEpoch 37/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.6953\nEpoch 38/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5798 - accuracy: 0.6966\nEpoch 39/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.6888\nEpoch 40/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.6901\nEpoch 41/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5851 - accuracy: 0.6875\nEpoch 42/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5861 - accuracy: 0.6888\nEpoch 43/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.7018\nEpoch 44/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.6953\nEpoch 45/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7005\nEpoch 46/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5709 - accuracy: 0.6953\nEpoch 47/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.6901\nEpoch 48/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.7005\nEpoch 49/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.6966\nEpoch 50/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5711 - accuracy: 0.7057\nEpoch 51/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7005\nEpoch 52/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.6888\nEpoch 53/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5569 - accuracy: 0.7031\nEpoch 54/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7018\nEpoch 55/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.6966\nEpoch 56/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7018\nEpoch 57/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5684 - accuracy: 0.6979\nEpoch 58/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.6914\nEpoch 59/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.6888\nEpoch 60/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5597 - accuracy: 0.6992\nEpoch 61/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.6966\nEpoch 62/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.6901\nEpoch 63/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.6927\nEpoch 64/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7031\nEpoch 65/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.6992\nEpoch 66/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.6966\nEpoch 67/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5674 - accuracy: 0.6862\nEpoch 68/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7031\nEpoch 69/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5514 - accuracy: 0.7031\nEpoch 70/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.7018\nEpoch 71/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7031\nEpoch 72/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5513 - accuracy: 0.6953\nEpoch 73/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5489 - accuracy: 0.6966\nEpoch 74/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.6966\nEpoch 75/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.6992\nEpoch 76/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5468 - accuracy: 0.6992\nEpoch 77/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7070\nEpoch 78/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5485 - accuracy: 0.7031\nEpoch 79/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5676 - accuracy: 0.6979\nEpoch 80/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.6979\nEpoch 81/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.7018\nEpoch 82/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7070\nEpoch 83/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.7044\nEpoch 84/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5723 - accuracy: 0.6940\nEpoch 85/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.7083\nEpoch 86/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.6979\nEpoch 87/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5518 - accuracy: 0.7044\nEpoch 88/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.6979\nEpoch 89/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7161\nEpoch 90/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.7044\nEpoch 91/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7018\nEpoch 92/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5480 - accuracy: 0.7109\nEpoch 93/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.6901\nEpoch 94/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7031\nEpoch 95/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7005\nEpoch 96/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.7070\nEpoch 97/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.6979\nEpoch 98/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.6992\nEpoch 99/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5407 - accuracy: 0.7096\nEpoch 100/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7148\nEpoch 101/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7044\nEpoch 102/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.6966\nEpoch 103/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7044\nEpoch 104/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7018\nEpoch 105/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5423 - accuracy: 0.7005\nEpoch 106/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7122\nEpoch 107/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.7096\nEpoch 108/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7018\nEpoch 109/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7122\nEpoch 110/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7044\nEpoch 111/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7005\nEpoch 112/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7135\nEpoch 113/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.7122\nEpoch 114/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7201\nEpoch 115/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7161\nEpoch 116/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7096\nEpoch 117/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.7109\nEpoch 118/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5385 - accuracy: 0.7083\nEpoch 119/150\n77/77 [==============================] - 0s 1ms/step - loss: 0.5355 - accuracy: 0.7057\nEpoch 120/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7031\nEpoch 121/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7044\nEpoch 122/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5237 - accuracy: 0.7188\nEpoch 123/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7096\nEpoch 124/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7031\nEpoch 125/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5378 - accuracy: 0.7044\nEpoch 126/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7174\nEpoch 127/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7031\nEpoch 128/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7096\nEpoch 129/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7031\nEpoch 130/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7018\nEpoch 131/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7018\nEpoch 132/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7161\nEpoch 133/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.7044\nEpoch 134/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7214\nEpoch 135/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.6992\nEpoch 136/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7070\nEpoch 137/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7057\nEpoch 138/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7096\nEpoch 139/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7057\nEpoch 140/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.7214\nEpoch 141/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5220 - accuracy: 0.7188\nEpoch 142/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7188\nEpoch 143/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7135\nEpoch 144/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.6953\nEpoch 145/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.7174\nEpoch 146/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7214\nEpoch 147/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7188\nEpoch 148/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7122\nEpoch 149/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7109\nEpoch 150/150\n77/77 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7214\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f9cc830f2d0>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "source": [
    "You have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset. This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy), but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset using the evaluate() function and pass it the same input and output used to train the model. This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy. The evaluate() function will return a list with two values. The first will be the loss of the model on the dataset, and the second will be the accuracy of the model on the dataset. You are only interested in reporting the accuracy so ignore the loss value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "24/24 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.7148\nAccuracy: 71.48\n"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print(\"Accuracy: %.2f\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}