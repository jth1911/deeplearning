{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1664252547362",
   "display_name": "Python 3.7.12 64-bit ('deeplearning': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Linear Regression with TensorFlow\n",
    "\n",
    "This next example comes from the introduction in the older version of TensorFlow tutorial. This example shows how you can define variables (e.g., W and b) as well as variables resulting from computation (y). Below, there is automatic differentiation under the hood. When we use `mse_loss()` function to compute the differences between y and y_data, there is a graph created connecting the values produced by the function (loss) to the TensorFlow variables W and b. TensorFlow uses this graph to deduce how to update the variables inside the minimize() function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "W:  [-0.9707446]\nb:  [0.]\n"
    }
   ],
   "source": [
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will figure that out for us.)\n",
    "W = tf.Variable(tf.random.normal([1]))      # set W to a random number\n",
    "b = tf.Variable(tf.zeros([1]))              # set b to 0\n",
    "\n",
    "print(\"W: \",W.numpy())\n",
    "print(\"b: \",b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute mean squared error between y_data and computed y\n",
    "def mse_loss():\n",
    "    y = W * x_data + b\n",
    "    loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 [-0.9697446] [0.001]\n500 [-0.5731104] [0.38337192]\n1000 [-0.38053682] [0.51465803]\n1500 [-0.28574288] [0.5054708]\n2000 [-0.20448416] [0.46435153]\n2500 [-0.12347975] [0.42062542]\n3000 [-0.04907935] [0.3804419]\n3500 [0.01190936] [0.34752312]\n4000 [0.05544063] [0.3240364]\n4500 [0.08155344] [0.30995017]\n5000 [0.09410743] [0.3031785]\n5500 [0.09865533] [0.30072534]\n6000 [0.09980205] [0.30010676]\n6500 [0.09998313] [0.30000907]\n7000 [0.09999857] [0.3000009]\n7500 [0.09999891] [0.3000007]\n8000 [0.09999914] [0.30000055]\n8500 [0.09999934] [0.3000004]\n9000 [0.09999947] [0.30000034]\n9500 [0.09999963] [0.30000025]\n"
    }
   ],
   "source": [
    "# Minimize the mean squared errors.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "for step in range(10000):\n",
    "    optimizer.minimize(mse_loss, var_list=[W,b])\n",
    "    if step % 500 == 0:\n",
    "        print(step, W.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}